---
title: "HW5"
author: "Amin Yakubu"
date: "4/24/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(ISLR)
library(mlbench)
library(caret)
library(e1071) 
```

# Data

```{r}
data(OJ)
seed = 58639

set.seed(seed)
rowTrain = createDataPartition(y = OJ$Purchase,
                                p = 0.747,
                                list = FALSE)

ctrl <- trainControl(method = "repeatedcv")
```

# Question A

```{r}
set.seed(seed)
svml.fit <- train(Purchase ~., 
                  data = OJ[rowTrain,], 
                  method = "svmLinear2",
                  preProcess = c("center", "scale"), 
                  tuneGrid = data.frame(cost = exp(seq(-10, 1, len = 50))),
                  trControl = ctrl)

ggplot(svml.fit, highlight = TRUE)

```

Training error using the training data

```{r}
pred.svml_training <- predict(svml.fit, newdata = OJ[rowTrain,])

confusionMatrix(data = pred.svml_training, 
                reference = OJ$Purchase[rowTrain])

linear_training_error_rate = mean(pred.svml_training != OJ$Purchase[rowTrain]) * 100
linear_training_error_rate
```

Trainig error rate is 15.5%

Test error using the held-out data

```{r}
pred.svml_testing <- predict(svml.fit, newdata = OJ[-rowTrain,])

confusionMatrix(data = pred.svml_testing, 
                reference = OJ$Purchase[-rowTrain])

linear_testing_error_rate = mean(pred.svml_testing != OJ$Purchase[-rowTrain]) * 100
linear_testing_error_rate
```

The testing error rate is 19.259%

# Question B

```{r}
svmr.grid <- expand.grid(C = exp(seq(-5, 2, len = 6)),
                         sigma = exp(seq(-6,-2, len = 6))) 
set.seed(seed)             
svmrad.fit <- train(Purchase ~., OJ, 
                  subset = rowTrain,
                  method = "svmRadial", 
                  preProcess = c("center", "scale"),
                  tuneGrid = svmr.grid,
                  trControl = ctrl)

ggplot(svmrad.fit, highlight = TRUE)
```

Now let's see what the training error rate is for the support vector machine with a radial kernel. 

```{r}
pred.svmrad_training <- predict(svmrad.fit, newdata = OJ[rowTrain,])

confusionMatrix(data = pred.svmrad_training, 
                reference = OJ$Purchase[rowTrain])

radial_training_error_rate = mean(pred.svmrad_training != OJ$Purchase[rowTrain]) * 100
radial_training_error_rate
```

The training error rate is 14.375%

Now let's find out the testing error rate

```{r}
pred.svmrad_testing <- predict(svmrad.fit, newdata = OJ[-rowTrain,])

confusionMatrix(data = pred.svmrad_testing, 
                reference = OJ$Purchase[-rowTrain])

radial_testing_error_rate = mean(pred.svmrad_testing != OJ$Purchase[-rowTrain]) * 100
radial_testing_error_rate
```

We expect the testing error to higher than the training error rate. The testing error rate is 18.5185%

# Question C

```{r}
resamp <- resamples(list(svmrad = svmrad.fit, svml = svml.fit))
summary(resamp)
bwplot(resamp)
```

In model selection we don't use the testing or training error, rather we use the cross validation error. Based on the median cross validation results from resamples, we see that the linear kernel has a higher a accuracy and seems to give better results, and therefore will be the preffered model in this case. Also, from the resamples summary, we see that the median cross validation error is slightly higher for the linear kernel method. The median kappa values are also very similar. 


