---
title: "Regression Trees and Ensemble Methods"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo = T, message = FALSE, results='hide', warning=FALSE}
library(ISLR)
library(caret)
library(rpart) # Make plot of the decision trees
library(rpart.plot)
library(party) # Conditional inference trees 
library(partykit) # Visualize the CIT 
library(randomForest) # implement the original RF. Was originally written in Fortran. Can be slow
library(ranger) # This is also for RF but was written in C++ and must faster
library(gbm) # For boosting models
library(plotmo)
library(pdp) # to create partial dependence plot
library(lime) # to provide interpreation on why a model gives a prediction. 
```

Predict a baseball player’s salary on the basis of various statistics associated with performance in the previous year. Use `?Hitters` for more details.

```{r}
data(Hitters)
Hitters2 <- Hitters[is.na(Hitters$Salary),] # players with missing outcome
Hitters <- na.omit(Hitters)
```

# Regression Trees
### The CART approach
We first apply the regression tree method to the Hitters data. `cp` is the complexity parameter. The default value for `cp` is 0.01.

```{r, fig.height = 3.5}
set.seed(1)
tree1 <- rpart(formula = Salary~., data = Hitters)
rpart.plot(tree1)
```

cp of 0.01 gives a large tree so we might consider prunning it further. 
In the terminal nodes, we have the mean values and the proportion of the data in the terminal nodes. So we have 142 as the mean and 27% of the data are there. 

We get a smaller tree by increasing the complexity parameter. When we increase alpha the size of the tree decreases because of a larger penalty term. 

```{r, fig.height=3, fig.width=4}
set.seed(1)
tree2 <- rpart(Salary~., Hitters, 
               control = rpart.control(cp = 0.1))
rpart.plot(tree2)
```


We next apply cost complexity pruning to obtain a tree with the right size. The functions `printcp()` and `plotcp()` give the set of possible cost-complexity prunings of a tree from a nested set. For the geometric means of the intervals of values of `cp` for which a pruning is optimal, a cross-validation has been done in the initial construction by `rpart()`. 

The `cptable` in the fit contains the mean and standard deviation of the errors in the cross-validated prediction against each of the geometric means, and these are plotted by `plotcp()`. `Rel error` (relative error) is \(1 – R^2\). The x-error is the cross-validation error generated by built-in cross validation. A good choice of `cp` for pruning is often the leftmost value for which the mean lies below the horizontal line.

```{r}
cpTable <- printcp(tree1) # This shows the cp values and the size of the tree ( the size is n split + 1). We don't look at the rel error cos as we increase the number it will also increase. The xerror is the cross validation error corresponding to the cp value. We want the smalled cross validation error. The xstd shows the standard deviation of the scores. By default it does a 10 fold cross validation. We see that nsplit of 8 to be the ideal. 
cpTable 
plotcp(tree1)
```

From the plot the minimun is 0.014. We also use the one standard error rule. Based on that we can choose 0.21. 

Prune the tree based on the `cp` table.

```{r, fig.align='center', fig.height=3, fig.width=3}
minErr <- which.min(cpTable[,4])
# minimum cross-validation error
tree3 <- prune(tree1, cp = cpTable[minErr,1])

# 1SE rule
tree4 <- prune(tree1, cp = cpTable[cpTable[,4] < cpTable[minErr,4] + cpTable[minErr,5],1][1])

rpart.plot(tree3)
rpart.plot(tree4)
```

Finally, the function `predict()` can be used for prediction from a fitted `rpart` object.

```{r}
predict(tree3, newdata = Hitters2[1:5,])
```

### Conditional inference trees

The implementation utilizes a unified framework for conditional inference, or permutation tests. Unlike CART, the stopping criterion is based on p-values. A split is implemented when (1 - p-value) exceeds the value given by `mincriterion` as specified in `ctree_control()`. This approach ensures that the right-sized tree is grown without additional pruning or cross-validation, but can stop early. At each step, the splitting variable is selected as the input variable with strongest association to the response (measured by a p-value corresponding to a test for the partial null hypothesis of a single input variable and the response). Such a splitting procedure can avoid a variable selection bias towards predictors with many possible cutpoints.

```{r, fig.width=10}
tree5 <- ctree(Salary~., Hitters) # This gives you are party object. 
plot(tree5)
```

Note that `tree5` is a `party` object. The function `predict()` can be used for prediction from a fitted `party` object.

```{r}
predict(tree5, newdata = Hitters2[1:5,])
```

## Using `caret`

There are two options for CART: tuning over `cp` and tuning over `maxdepth`.

```{r, fig.height=2.8, fig.width=3.5, fig.show='hold'}
ctrl <- trainControl(method = "cv") # Also good to do repeated cv. Can be slow

set.seed(1)
# tune over cp, method = "rpart"
rpart.fit <- train(Salary~., Hitters, 
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-6,-2, length = 20))),
                   trControl = ctrl)
ggplot(rpart.fit, highlight = TRUE)
rpart.plot(rpart.fit$finalModel)
```

```{r, fig.height=2.8, fig.width=3.5, fig.show='hold'}
set.seed(1)
# tune over maximum depth, method = "rpart2"
rpart2.fit <- train(Salary~., Hitters, 
                   method = "rpart2",
                   tuneGrid = data.frame(maxdepth = 1:7),
                   trControl = ctrl)
ggplot(rpart2.fit, highlight = TRUE)
rpart.plot(rpart2.fit$finalModel)
```

We can also fit a conditional inference tree model. The tuning parameter is `mincriterion`. 
Here the p value is the tuning parameter.

```{r}
set.seed(1)
ctree.fit <- train(Salary~., Hitters, 
                   method = "ctree",
                   tuneGrid = data.frame(mincriterion = 1-exp(seq(-6, -2, length = 20))),
                   trControl = ctrl)
ggplot(ctree.fit, highlight = TRUE)
plot(ctree.fit$finalModel)
```


# Ensemble methods

## Bagging and Random forests

The function `randomForest()` implements Breiman's random forest algorithm (based on Breiman and Cutler's original Fortran code) for classification and regression. `ranger()` is a fast implementation of Breiman's random forests, particularly suited for high dimensional data.

```{r}
set.seed(1)
bagging <- randomForest(Salary~., Hitters,
                   mtry = 19) # We have 19 predictors. when it is equal to the number of predictors. Then we are doing bagging

set.seed(1)
rf <- randomForest(Salary~., Hitters,
                   mtry = 6) # Rule of thumb for m is p/3

# fast implementation
set.seed(1)
rf2 <- ranger(Salary~., Hitters,
              mtry = 6) 

# scale permutation importance by standard error 

predict(rf, newdata = Hitters2[1:5,])
predict(rf2, data = Hitters2[1:5,])$predictions # in ranger there's a subobject called predictions
```


## Boosting

We first fit a gradient boosting model with Gaussian loss function with 10000 iterations.

```{r}
set.seed(1)
bst <- gbm(Salary~., Hitters,
           distribution = "gaussian",
           n.trees = 5000, 
           interaction.depth = 3,
           shrinkage = 0.005, # This is the learning rate. Smaller is better. Small lr need large tree. (consider trade off)
           cv.folds = 10)
```

There's a built in cross validation in gbm. 

We plot loss function as a result of number of trees added to the ensemble.

```{r}
nt <- gbm.perf(bst, method = "cv")
nt
```

The plot shows the performance of the fitted model as a function of the number of trees. The green line is the cross validation error and the black line is the training error. The dash line shows the optimal
This only tunes the number of trees. 

## Grid search using `caret`

We use the fast implementation of random forest when tuning the model.

```{r}
# Try more if possible
rf.grid <- expand.grid(mtry = 1:6, # number of features you choose to split 
                       splitrule = "variance",
                       min.node.size = 1:6)
set.seed(1)
rf.fit <- train(Salary~., Hitters, 
                method = "ranger", # the fast implementation of rf. it's better cos it allows three tuning parameters
                tuneGrid = rf.grid,
                trControl = ctrl)

ggplot(rf.fit, highlight = TRUE)
```

We can see that the rf is not very sensitive to tuning. That's an advantage cos u don't have to tune carefully unlike boosting.

We then tune the `gbm` model.

```{r}
# Try more 
gbm.grid <- expand.grid(n.trees = c(2000,3000),
                        interaction.depth = 2:10, # usually we don't go beyond 10
                        shrinkage = c(0.001,0.003,0.005),
                        n.minobsinnode = 1 ) # the min number of observations in your node 
set.seed(1)
gbm.fit <- train(Salary~., Hitters, 
                 method = "gbm",
                 tuneGrid = gbm.grid,
                 trControl = ctrl,
                 verbose = FALSE)

ggplot(gbm.fit, highlight = TRUE)
```

As you can see, it takes a while to train the `gbm` even with a rough tuning grid. The `xgboost` package provides an efficient implementation of gradient boosting framework (apprx 10x faster than `gbm`). You can find much useful information here: https://github.com/dmlc/xgboost/tree/master/demo.

Compare the cross-validation performance. You can also compare with other models that we fitted before.

```{r}
resamp <- resamples(list(rf = rf.fit, gbm = gbm.fit))
summary(resamp)
```

## Explain the black-box models

### Variable importance

We can extract the variable importance from the fitted models. In what follows, the first measure is computed from permuting OOB data. The second measure is the total decrease in node impurities from splitting on the variable, averaged over all trees. For regression, node impurity is measured by residual sum of squares.

```{r}
set.seed(1)
rf2.final.per <- ranger(Salary~., Hitters,
                        mtry = 3, splitrule = "variance",
                        min.node.size = 5,
                        importance = "permutation", 
                        scale.permutation.importance = TRUE) 

# mtry and min.node.size number are chosen after tuning

barplot(sort(ranger::importance(rf2.final.per), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(19))

set.seed(1)
rf2.final.imp <- ranger(Salary~., Hitters,
                        mtry = 3, splitrule = "variance",
                        min.node.size = 5,
                        importance = "impurity")  

# impurity is another way to computing the importance 

barplot(sort(ranger::importance(rf2.final.imp), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(19))

```


The variables with negative importance signifies including them can even increase the error. 

Variable importance from boosting can be obtained using the `summary()` function.

```{r}
summary(gbm.fit$finalModel, las = 2, cBars = 19, cex.names = 0.6)
```

### Partial dependence plots and individual conditional expectation curves

After the most relevant variables have been identified, the next step is to attempt to understand how the response variable changes based on these variables. For this we can use partial dependence plots (PDPs) and individual conditional expectation (ICE) curves.

PDPs plot the change in the average predicted value as specified feature(s) vary over their marginal distribution. The PDP plot below displays the average change in predicted `Salary` as we vary `CRBI` while holding all other variables constant. This is done by holding all variables constant for each observation in our training data set but then apply the unique values of `CRBI` for each observation. We then average the `Salary` across all the observations. 

```{r}
pdp.rf <- rf.fit %>% 
  partial(pred.var = "CRBI", 
          grid.resolution = 100) %>%
  autoplot(rug = TRUE, train = Hitters) +
  ggtitle("Random forest") 

pdp.gbm <- gbm.fit %>% 
  partial(pred.var = "CRBI", 
          grid.resolution = 100) %>%
  autoplot(rug = TRUE, train = Hitters) +
  ggtitle("Boosting") 

grid.arrange(pdp.rf, pdp.gbm, nrow = 1)
```

ICE curves are an extension of PDP plots but, rather than plot the average marginal effect on the response variable, we plot the change in the predicted response variable for each observation as we vary each predictor variable. 

```{r}
ice1.rf <- rf.fit %>% 
  partial(pred.var = "CRBI", 
          grid.resolution = 100,
          ice = TRUE) %>%
  autoplot(train = Hitters, alpha = .1) +
  ggtitle("Random forest, non-centered") 

ice2.rf <- rf.fit %>% 
  partial(pred.var = "CRBI", 
          grid.resolution = 100,
          ice = TRUE) %>%
  autoplot(train = Hitters, alpha = .1, 
           center = TRUE) +
  ggtitle("Random forest, centered") 

ice1.gbm <- gbm.fit %>% 
  partial(pred.var = "CRBI", 
          grid.resolution = 100,
          ice = TRUE) %>%
  autoplot(train = Hitters, alpha = .1) +
  ggtitle("Boosting, non-centered") 

ice2.gbm <- gbm.fit %>% 
  partial(pred.var = "CRBI", 
          grid.resolution = 100,
          ice = TRUE) %>%
  autoplot(train = Hitters, alpha = .1, 
           center = TRUE) +
  ggtitle("Boosting, centered") 

grid.arrange(ice1.rf, ice2.rf, ice1.gbm, ice2.gbm,
             nrow = 2, ncol = 2)
```


### Plot the features in an explanation
The function `plot_features()` creates a compact visual representation of the explanations for each case and label combination in an explanation. Each extracted feature is shown with its weight, thus giving the importance of the feature in the label prediction.

```{r, warning=FALSE}
new_obs <- Hitters2[3,-19]
explainer.gbm <- lime(Hitters[,-19], gbm.fit)
explanation.gbm <- explain(new_obs, explainer.gbm, n_features = 19)
plot_features(explanation.gbm)
plot_explanations(explanation.gbm)
```

```{r, warning=FALSE}
explainer.rf <- lime(Hitters[,-19], rf.fit)
explanation.rf <- explain(new_obs, explainer.rf, n_features = 19)
plot_features(explanation.rf)
```



