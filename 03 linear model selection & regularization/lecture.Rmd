---
title: "Ridge Regression, Lasso & Dimention reduction methods"
author: "Amin Yakubu"
date: "2/13/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo = T, message = FALSE, results='hide', warning=FALSE}
library(ISLR)
library(glmnet) # For lasso, ridge & elastic net
library(caret) # for train function
library(corrplot) # For correlation plot
library(plotmo) # for trace plot. glmnet can also produce plots but this one is better
library(pls)
```

Predict a baseball player’s salary on the basis of various statistics associated with performance in the previous year. Use `?Hitters` for more details.

```{r}
data(Hitters)
# delete rows containing the missing data
Hitters <- na.omit(Hitters)

# matrix of predictors (glmnet uses input matrix). We specify the response and use . the specify all predictors
x <- model.matrix(Salary ~ . , Hitters)[,-1] # we delete the intercept (which is the first column)

# vector of response
y <- Hitters$Salary

corrplot(cor(x), method = 'square') # method is optional

```

## Ridge regression using `glmnet()`

`alpha` is the elasticnet mixing parameter. The penalty on the coefficient vetor for predictor $j$ is $(1-\alpha)/2||\beta_j||_2^2+\alpha||\beta_j||_1$. `alpha=1` is the lasso penalty, and `alpha=0` the ridge penalty. `glmnet()` function standardizes the variables by default. `ridge.mod` contains the coefficient estimates for a set of lambda values. The grid for lambda is in `ridge.mod$lambda`. 

```{r}
# fit the ridge regression (alpha = 0) with a sequence of lambdas
ridge.mod <- glmnet(x, y, alpha = 0, lambda = exp(seq(-1, 10, length = 100))) # 100 you will have 100 solutions. 
# default here is standardized. 
# set alpha = 0 for ridge
# set alpha = 1 for lasso
# lambda is trying different values. Alternatively use nlambda = 100. It will choose the values automatically
# another function standardize = TRUE meaning you will center you predictors and outcome

ridge.mod$lambda
```

`coef(ridge.mod)` gives the coefficient matrix. Each column is the fit corresponding to one lambda value.

```{r}
mat.coef <- coef(ridge.mod)
dim(mat.coef)
```

### Cross-validation

We use cross-validation to determine the optimal value of lambda. The two vertical lines are the for minimal MSE and 1SE rule. The 1SE rule gives the model with fewest coefficients that's less than one SE away from the sub-model with the lowest error.
```{r}
set.seed(2)
cv.ridge <- cv.glmnet(x, y, 
                      alpha = 0, 
                      lambda = exp(seq(-1, 10, length = 100)), 
                      type.measure = "mse")

# here alpha = 0, meaning ridge
# type.measure the metric to decide the best model. 

plot(cv.ridge)
# the redline is the mse for each lambda value
# we also see the mse + SE and mse - SE
# the first dashed line shows the optimal value for lambda
# second dashed line is the 1 standared error rule - gives the model with the fewest coefficients. Another good model esp if you want few parameters. It depends on your own choice. They are both good.
```

### Trace plot

There are two functions for generating the trace plot.
```{r}
plot(ridge.mod, xvar = "lambda", label = TRUE)

plot_glmnet(ridge.mod, xvar = "rlambda", label = 19) # label = 10 is the defaul
```

### Coefficients of the final model

Get the coefficients of the optimal model. `s` is value of the penalty parameter `lambda` at which predictions are required.
```{r}
best.lambda <- cv.ridge$lambda.min # cv.ridge$labmda.1se gives you the other optimal value on the other side of the plot
best.lambda

predict(ridge.mod, s = best.lambda, type = "coefficients") # the coefficents are on the original scale -- not scaled
# use type = 'response' for homework. and also use newx = to a matrix. So it will return a vector of predicted responses
```


## Lasso using `glmnet()`

The syntax is along the same line as ridge regression. Now we use `alpha = 1`.
```{r}
cv.lasso <- cv.glmnet(x,y, alpha = 1, lambda = exp(seq(-1, 5, length = 100)))
cv.lasso$lambda.min

# alpha = 1 is lasso and alpha = 0 is ridge

```


```{r}
plot(cv.lasso)

# shows the number of non zero coefficents. Look at the numbers on top of the model
```


```{r}
# cv.lasso$glmnet.fit is a fitted glmnet object for the full data
# You can also plot the result obtained from glmnet()
plot(cv.lasso$glmnet.fit, xvar = "lambda", label = TRUE)
plot_glmnet(cv.lasso$glmnet.fit)
```


```{r}
predict(cv.lasso, s = "lambda.min", type = "coefficients")
# you can also use the actual number for s
```

## Ridge and lasso using `caret`

```{r}
ctrl1 <- trainControl(method = "repeatedcv", number = 10, repeats = 5)
# you can try other options

set.seed(2)
ridge.fit <- train(x, y,
                     method = "glmnet",
                     tuneGrid = expand.grid(alpha = 0, 
                                            lambda = exp(seq(-1, 10, length = 100))),
                   # preProc = c("center", "scale"),
                     trControl = ctrl1)
# preProc was not used because that's the default for glmnet. Other methods it may not be the default

plot(ridge.fit)
plot(ridge.fit, xTrans = function(x) log(x)) # here were are plotting log lambda so it looks like the previous plots

names(ridge.fit)
ridge.fit$bestTune

coef(ridge.fit$finalModel, ridge.fit$bestTune$lambda)

# you can use predict on the ridge.fit$finalModel. Because ridge.fit$finalModel is a glmnet object
```

```{r}
set.seed(2)
lasso.fit <- train(x, y,
                     method = "glmnet",
                     tuneGrid = expand.grid(alpha = 1, 
                                            lambda = exp(seq(-1, 5, length = 100))),
                   # preProc = c("center", "scale"),
                     trControl = ctrl1)

plot(lasso.fit, xTrans = function(x) log(x))

lasso.fit$bestTune

coef(lasso.fit$finalModel,lasso.fit$bestTune$lambda)
```

```{r}
set.seed(2)
enet.fit <- train(x, y,
                     method = "glmnet",
                     tuneGrid = expand.grid(alpha = seq(0, 1, length = 5), 
                                            # We are seeing if alpha between 0 and 1 is better
                                            lambda = exp(seq(-2, 4, length = 50))),
                   # preProc = c("center", "scale"),
                     trControl = ctrl1)

# for each alpha we have 50 lambdas. 
enet.fit$bestTune

ggplot(enet.fit)
```


```{r, fig.width=5}
set.seed(2)
lm.fit <- train(x, y,
                method = "lm",
                trControl = ctrl1)

resamp <- resamples(list(lasso = lasso.fit, ridge = ridge.fit, lm = lm.fit, enet = enet.fit)) # here we can compare the models
summary(resamp)

parallelplot(resamp, metric = "RMSE") # There are 50 curves showing the performances of the 4 models

bwplot(resamp, metric = "RMSE") # here we used a box plot
```


# Dimension Reduction Methods


Predict a baseball player’s salary on the basis of various statistics associated with performance in the previous year. Use `?Hitters` for more details. 

Ideally, a model should be evaluated on datasets that were not used to build or fine-tune the model, so that they provide an unbiased sense of model effectiveness. When a large amount of data is at hand, a set of samples can be set aside to evaluate the final model. However, when the number of samples is not large, a test set may be avoided because every sample may be needed for model building. Moreover, the size of the test set may not have sufficient power or precision to make reasonable judgements.

Last time we used all the data to build the models. This time we split the data into a training set and a test set. 

```{r}
data(Hitters)
# delete rows containing the missing data
Hitters <- na.omit(Hitters)

set.seed(2019)
trRows <- createDataPartition(Hitters$Salary,
                              p = .75,
                              list = F)

# training data
# matrix of predictors (glmnet uses input matrix)
x <- model.matrix(Salary~.,Hitters)[trRows,-1]
# vector of response
y <- Hitters$Salary[trRows]

# test data
x2 <- model.matrix(Salary~.,Hitters)[-trRows,-1]
y2 <- Hitters$Salary[-trRows]
```

## Principal components regression (PCR)

We fit the PCR model using the function `pcr()`.

```{r}
set.seed(2)
pcr.mod <- pcr(Salary~., 
               data = Hitters[trRows,],
               scale = TRUE, 
               validation = "CV")

summary(pcr.mod)

validationplot(pcr.mod, val.type = "MSEP", legendpos = "topright")

predy2.pcr <- predict(pcr.mod, newdata = Hitters[-trRows,], 
                      ncomp = 18)
# test MSE
mean((predy2.pcr - y2)^2)
```


## Partial least squares (PLS)

We fit the PLS model using the function `plsr()`.
```{r}
set.seed(2)
pls.mod <- plsr(Salary~., 
                data = Hitters[trRows,], 
                scale = TRUE,  
                validation = "CV")

summary(pls.mod)
validationplot(pls.mod, val.type="MSEP", legendpos = "topright")

predy2.pls <- predict(pcr.mod, newdata = Hitters[-trRows,], 
                      ncomp = 15)
# test MSE
mean((predy2.pls-y2)^2)
```

## PCR and PLS using `caret`

### PCR 
```{r}
ctrl1 <- trainControl(method = "repeatedcv", number = 10, repeats = 5)

# Two ways for standardizing predictors
# train(..., preProc = c("center", "scale"))
set.seed(2)
pcr.fit <- train(x, y,
                 method = "pcr",
                 tuneLength = 18, # number of components or specifiy tunegrid 1 to 18. 
                 trControl = ctrl1,
                 preProc = c("center", "scale"))

# need to preprocess your data when using predict()
trans <- preProcess(x, method = c("center", "scale"))

predy2.pcr2 <- predict(pcr.fit$finalModel, newdata = predict(trans, x2), 
                       ncomp = pcr.fit$bestTune$ncomp)

mean((predy2.pcr2 - y2)^2)

# pcr(..., scale = TRUE)
set.seed(2)
pcr.fit2 <- train(x, y,
                  method = "pcr",
                  tuneLength = 18,
                  trControl = ctrl1,
                  scale = TRUE) # When scale = True. There's no need for preprocessing

predy2.pcr3 <- predict(pcr.fit2$finalModel, newdata = x2, 
                       ncomp = pcr.fit2$bestTune$ncomp)
mean((predy2.pcr3-y2)^2)

ggplot(pcr.fit, highlight = TRUE) + theme_bw()
# ggplot(pcr.fit2, highlight = TRUE) # the same plot
```

### PLS
```{r}
set.seed(2)
pls.fit <- train(x, y,
                 method = "pls",
                 tuneLength = 18,
                 trControl = ctrl1,
                 scale = TRUE)
predy2.pls2 <- predict(pls.fit$finalModel, newdata = x2, 
                       ncomp = pls.fit$bestTune$ncomp)
mean((predy2.pls2-y2)^2)

ggplot(pls.fit, highlight = TRUE)
```

Here are some old codes on ridge, lasso and ordinary least squares.
```{r}
set.seed(2)
ridge.fit <- train(x, y,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 0, 
                                          lambda = exp(seq(-1, 10, length=100))),
                   # preProc = c("center", "scale"),
                   trControl = ctrl1)
predy2.ridge <- predict(ridge.fit$finalModel, newx = x2, 
                        s = ridge.fit$bestTune$lambda, type = "response")
mean((predy2.ridge-y2)^2)

set.seed(2)
lasso.fit <- train(x, y,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(-1, 5, length=100))),
                   # preProc = c("center", "scale"),
                   trControl = ctrl1)
predy2.lasso <- predict(lasso.fit$finalModel, newx = x2, 
                        s = lasso.fit$bestTune$lambda, type = "response")
mean((predy2.lasso-y2)^2)

set.seed(2)
lm.fit <- train(x, y,
                method = "lm",
                trControl = ctrl1)
predy2.lm <- predict(lm.fit$finalModel, newdata = data.frame(x2))
mean((predy2.lm-y2)^2)

```

Comparing the models based on resampling results.
```{r}
resamp <- resamples(list(lasso = lasso.fit, 
                         ridge = ridge.fit, 
                         pcr = pcr.fit, 
                         pls = pls.fit,
                         lm = lm.fit))
summary(resamp)

bwplot(resamp, metric = "RMSE")
```



