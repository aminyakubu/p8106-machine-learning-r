---
title: "HW4"
author: "Amin Yakubu"
date: "4/19/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(lasso2)
library(ISLR)
library(caret)
library(rpart)
library(rpart.plot)
library(party) 
library(partykit) 
library(randomForest) 
library(ranger) 
library(gbm) 
library(plotmo)
library(pdp) 
library(lime)
```

# Question 1a

```{r}
seed = 1
data("Prostate")
ctrl <- trainControl(method = "cv")
```

Here, I fit a regression tree with lpsa as the response and the other variables as predictors, and then use cross-validation to determine the optimal tree size. I'll tune over complexity parameter. 

```{r}
set.seed(seed)

tree <- rpart(formula = lpsa ~., data = Prostate,
              control = rpart.control(cp = 0.001))
rpart.plot(tree)

cpTable <- printcp(tree)
plotcp(tree)

minErr <- which.min(cpTable[,4])
```

Tree size of 8 corresponds to the lowest cross validation error. 

```{r}
# Tree size = nsplit + 1
cpTable[cpTable[,4] < cpTable[minErr,4] + cpTable[minErr,5], 2][1] + 1 
```

The tree size obtained using the 1 SE rule is tree of size 4. We can also see that from the plot since tree size of 4 is the leftmost value below the horizontal line. 

There trees are different. 

# Question 1b

I'll select and prune my tree using the 1 SE 

```{r}
selected_tree = prune(tree, cp = cpTable[cpTable[,4] < cpTable[minErr,4] + cpTable[minErr,5], 1][1])
rpart.plot(selected_tree)
```

The mean `lpsa` for observations with less than 2.5 of `lcavol` and further less than -0.48 of `lcavol` is 0.6. 9% of the total observations are in this terminal node. 

# Question 1c - Bagging

```{r}
bagging.grid <- expand.grid(mtry = 8, 
                       splitrule = "variance",
                       min.node.size = 1:20) 
set.seed(seed)
bagging <- train(lpsa~., Prostate, 
                method = "ranger",
                tuneGrid = bagging.grid,
                trControl = ctrl,
                importance = 'permutation')

ggplot(bagging, highlight = TRUE)

barplot(sort(ranger::importance(bagging$finalModel), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(19))

bagging$results[which.min(bagging$results[,5]),]
```

# Question 1d - Random Forest

```{r}
rf.grid = expand.grid(mtry = 1:7, 
                       splitrule = "variance",
                       min.node.size = 1:20) 
set.seed(seed)
rf.fit = train(lpsa~., Prostate, 
                method = "ranger",
                tuneGrid = rf.grid,
                trControl = ctrl,
                importance = 'permutation')

ggplot(rf.fit, highlight = TRUE)

barplot(sort(ranger::importance(rf.fit$finalModel), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(19))

rf.fit$results[which.min(rf.fit$results[,5]),]
```

# Question 1e

```{r}
gbm.grid = expand.grid(n.trees = c(2000,3000, 5000),
                        interaction.depth = 2:10, 
                        shrinkage = c(0.01, 0.001,0.003,0.005),
                        n.minobsinnode = 1)
set.seed(seed)
gbm.fit = train(lpsa ~., Prostate, 
                 method = "gbm",
                 tuneGrid = gbm.grid,
                 verbose = FALSE,
                 trControl = ctrl)

ggplot(gbm.fit, highlight = TRUE)
```

```{r}
summary(gbm.fit$finalModel, las = 2, cBars = 19, cex.names = 0.6)
```

```{r}
gbm.fit$results[which.min(gbm.fit$results[,5]),]
```

# Question 1f

```{r}
resamp = resamples(list(rf = rf.fit, gbm = gbm.fit, bagging = bagging))
summary(resamp)
```

# Question 2a

```{r}
data("OJ")
```

```{r}
set.seed(seed)
rowTrain = createDataPartition(y = OJ$Purchase,
                                p = 0.747,
                                list = FALSE)

ctrl <- trainControl(method = "repeatedcv")
                    # summaryFunction = twoClassSummary,
                     # classProbs = TRUE)

```

```{r}
set.seed(seed)
rpart.class <- train(Purchase ~., OJ, 
                   subset = rowTrain,
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-7,-2, len = 50))),
                   trControl = ctrl,
                   metric = "Accuracy")

ggplot(rpart.class, highlight = T)
rpart.plot(rpart.class$finalModel)
```

```{r}
rpart.pred <- predict(rpart.class, newdata = OJ[-rowTrain,])

confusionMatrix(rpart.pred,
                reference = OJ$Purchase[-rowTrain])

# Error rate
cat(c("The error rate for the GBM model is", mean(rpart.pred != OJ$Purchase[-rowTrain])))
```

# Question 2b

```{r}
rf.grid <- expand.grid(mtry = 1:10,
                       splitrule = "gini",
                       min.node.size = 1:6)
set.seed(seed)
rf.class <- train(Purchase ~., OJ, 
                subset = rowTrain,
                method = "ranger",
                tuneGrid = rf.grid,
                metric = "Accuracy",
                trControl = ctrl,
                importance = 'permutation')

ggplot(rf.class, highlight = TRUE)

rf.pred = predict(rf.class, newdata = OJ[-rowTrain,])

confusionMatrix(rf.pred,
                reference = OJ$Purchase[-rowTrain])

# Error rate
cat(c("The error rate for the GBM model is", mean(rf.pred != OJ$Purchase[-rowTrain])))
```

# Question 3c

```{r}
gbmB.grid <- expand.grid(n.trees = c(2000,3000,4000),
                        interaction.depth = 1:6,
                        shrinkage = c(0.001,0.003,0.005),
                        n.minobsinnode = 1)
set.seed(seed)
gbmB.fit <- train(Purchase ~., OJ, 
                 subset = rowTrain, 
                 tuneGrid = gbmB.grid,
                 trControl = ctrl,
                 method = "gbm",
                 distribution = "adaboost",
                 metric = "Accuracy",
                 verbose = FALSE)

ggplot(gbmB.fit, highlight = TRUE)

gbm.pred = predict(gbmB.fit, newdata = OJ[-rowTrain,])

confusionMatrix(gbm.pred,
                reference = OJ$Purchase[-rowTrain])

# Error rate
cat(c("The error rate for the GBM model is", mean(gbm.pred != OJ$Purchase[-rowTrain])))
```

