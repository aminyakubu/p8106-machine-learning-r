---
title: "HW4"
author: "Amin Yakubu"
date: "4/19/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(lasso2)
library(ISLR)
library(caret)
library(rpart)
library(rpart.plot)
library(party) 
library(partykit) 
library(randomForest) 
library(ranger) 
library(gbm) 
library(plotmo)
library(pdp) 
library(lime)
```

# Question 1a

```{r}
seed = 1
data("Prostate")
ctrl <- trainControl(method = "cv")
```

Here, I fit a regression tree with lpsa as the response and the other variables as predictors, and then use cross-validation to determine the optimal tree size. I'll tune over complexity parameter. 

```{r}
set.seed(seed)

tree <- rpart(formula = lpsa ~., data = Prostate,
              control = rpart.control(cp = 0.001))

cpTable <- printcp(tree)
plotcp(tree)

minErr <- which.min(cpTable[,4])
```

Tree size of 8 corresponds to the lowest cross validation error. 

Checking tree size which corresponds to the 1SE rule

```{r}
# Tree size = nsplit + 1
cpTable[cpTable[,4] < cpTable[minErr,4] + cpTable[minErr,5], 2][1] + 1 
```

The tree size obtained using the 1 SE rule is tree of size 4. We can also see that from the plot since tree size of 4 is the leftmost value below the horizontal line. The tree corresponding to the lowest cross validation error (size 8) is different from the tree corresponding to the 1 SE rule (size 4)

# Question 1b

I'll select and prune my tree using the 1 SE 

```{r}
selected_tree = prune(tree, cp = cpTable[cpTable[,4] < cpTable[minErr,4] + cpTable[minErr,5], 1][1])
rpart.plot(selected_tree)
```

The mean `lpsa` for observations with less than 2.5 of `lcavol` and further less than -0.48 of `lcavol` is 0.6. 9% of the total observations are in this terminal node. 

# Question 1c - Bagging

```{r}
bagging.grid <- expand.grid(mtry = 8, 
                       splitrule = "variance",
                       min.node.size = 1:20) 
set.seed(seed)
bagging <- train(lpsa~., Prostate, 
                method = "ranger",
                tuneGrid = bagging.grid,
                trControl = ctrl,
                importance = 'permutation')

ggplot(bagging, highlight = TRUE)

barplot(sort(ranger::importance(bagging$finalModel), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(19))

bagging$results[which.min(bagging$results[,5]),]
```

From the variable important plot, we see that the 3 most important variables are `lcavol`, `lweight` and `svi`. 

# Question 1d - Random Forest

I'll randomly select between 1 to 7 of the variables for each split to see which number works best.  

```{r}
rf.grid = expand.grid(mtry = 1:7, 
                       splitrule = "variance",
                       min.node.size = 1:20) 
set.seed(seed)
rf.fit = train(lpsa~., Prostate, 
                method = "ranger",
                tuneGrid = rf.grid,
                trControl = ctrl,
                importance = 'permutation')

ggplot(rf.fit, highlight = TRUE)

barplot(sort(ranger::importance(rf.fit$finalModel), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(19))

# rf.fit$results[which.min(rf.fit$results[,5]),]
```

The best model used has `mtry` of 4 and `min.node.size` of 14 The result of the random forest shows that the most important variables are `lcavol`, `svi` and `lweight`. In this case `svi` is more important that `lweight`. 

# Question 1e

```{r}
gbm.grid = expand.grid(n.trees = c(2000,3000, 5000),
                        interaction.depth = 2:10, 
                        shrinkage = c(0.01, 0.001,0.003,0.005),
                        n.minobsinnode = 1)
set.seed(seed)
gbm.fit = train(lpsa ~., Prostate, 
                 method = "gbm",
                 tuneGrid = gbm.grid,
                 verbose = FALSE,
                 trControl = ctrl)

ggplot(gbm.fit, highlight = TRUE)
```

The selected final model has 5000 trees with a depth of 2 with a learning rate (shrinkage) of 0.001. 

```{r}
summary(gbm.fit$finalModel, las = 2, cBars = 19, cex.names = 0.6)

# gbm.fit$results[which.min(gbm.fit$results[,5]),]
```

The most important variables using boosting are `lcavol`, `lweight` and `svi`. The order is similar to the bagging method. 

# Question 1f

```{r}
resamp = resamples(list(rf = rf.fit, gbm = gbm.fit, bagging = bagging))
summary(resamp)

bwplot(resamp, metric = "RMSE")
```

I'll choose the regression tree method to predict PSA level. This is because we see that the cross validation is smallest for the bagging method (both using the 1SE rule and the mininum cross validation error). Also, in addition to having the smallest cross validation error it is easier to explain (more interpretable). 

# Question 2a - Classification Tree

```{r}
data("OJ")
```

```{r}
set.seed(seed)
rowTrain = createDataPartition(y = OJ$Purchase,
                                p = 0.747,
                                list = FALSE)

ctrl <- trainControl(method = "repeatedcv")

```

Since we are interested in missclassification rate, I'll use accuracy as the metric in the cross validation to select the model.

```{r}
set.seed(seed)
rpart.class <- train(Purchase ~., OJ, 
                   subset = rowTrain,
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-7,-2, len = 50))),
                   trControl = ctrl,
                   metric = "Accuracy")

ggplot(rpart.class, highlight = T)
rpart.plot(rpart.class$finalModel)

rpart.class$bestTune
```

The best tree size is 4 (number of splits + 1) which corresponds to a complexity parameter of 0.01947204. 

Now let's use the model to predict the test data. 

```{r}
rpart.pred <- predict(rpart.class, newdata = OJ[-rowTrain,])

confusionMatrix(rpart.pred,
                reference = OJ$Purchase[-rowTrain])

# Error rate
error_rate = mean(rpart.pred != OJ$Purchase[-rowTrain]) * 100

cat(c("The error rate for the classification tree is", error_rate, '%'))
```

# Question 2b - Random Forest

```{r}
rf.grid <- expand.grid(mtry = 1:10,
                       splitrule = "gini",
                       min.node.size = 1:6)
set.seed(seed)
rf.class <- train(Purchase ~., OJ, 
                subset = rowTrain,
                method = "ranger",
                tuneGrid = rf.grid,
                metric = "Accuracy",
                trControl = ctrl,
                importance = 'permutation')

ggplot(rf.class, highlight = TRUE)

```

The selected model using cross validation has `mtry` of 5 `and min.node.size` of 5 as well. 

Variable Importance
```{r}
barplot(sort(ranger::importance(rf.class$finalModel), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(19))
```

From the variable important plot, we see that the variable `LoyalCH` is the most important. 

Now let's use the random forest model selected using cross validation to predict the outcome for the test data

```{r}
rf.pred = predict(rf.class, newdata = OJ[-rowTrain,])

confusionMatrix(rf.pred,
                reference = OJ$Purchase[-rowTrain])

# Error rate
error_rate = mean(rf.pred != OJ$Purchase[-rowTrain]) * 100

cat(c("The error rate for the random forest model is", error_rate, '%'))
```

# Question 3c - Boosting

```{r}
gbmB.grid <- expand.grid(n.trees = c(2000,3000,4000),
                        interaction.depth = 1:6,
                        shrinkage = c(0.001,0.003,0.005),
                        n.minobsinnode = 1)
set.seed(seed)
gbmB.fit <- train(Purchase ~., OJ, 
                 subset = rowTrain, 
                 tuneGrid = gbmB.grid,
                 trControl = ctrl,
                 method = "gbm",
                 distribution = "adaboost",
                 metric = "Accuracy",
                 verbose = FALSE)

ggplot(gbmB.fit, highlight = TRUE)
```

The selected model had a maximum tree depth of 3 and learning rate of 0.003 with 3000 boosting iterations. 

```{r}
summary(gbmB.fit$finalModel, las = 2, cBars = 19, cex.names = 0.6)
```

The variable importance is very similar that of random forest in that the most important variable is `LoyalCH`. 

Next, I'll predict and compute the missclassification rate of the boosted model.

```{r}
gbm.pred = predict(gbmB.fit, newdata = OJ[-rowTrain,])

confusionMatrix(gbm.pred,
                reference = OJ$Purchase[-rowTrain])

# Error rate
error_rate = mean(gbm.pred != OJ$Purchase[-rowTrain]) * 100

cat(c("The error rate for the GBM model is", error_rate,'%'))
```

The missclassification rate for the GBM model (15.55%) is much better than the classification tree (18.518%) and the random forest (19.2592%). 



